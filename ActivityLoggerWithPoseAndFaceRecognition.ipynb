{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70c27b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from deepface import DeepFace\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bacd730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "face_db_path = \"face_db\"\n",
    "model_name = \"VGG-Face\"\n",
    "threshold = 0.4\n",
    "smoothing_decay = 0.1\n",
    "activity_queue = deque(maxlen=10)\n",
    "prev_left_hand, prev_right_hand = None, None\n",
    "last_label, last_distance = None, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9808378c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The layer sequential_148 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Load model only once ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[INFO] Loading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mDeepFace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# === Utility function to check valid image ===\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_valid_image\u001b[39m(path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev.Solanki\\Desktop\\CV Project\\env\\Lib\\site-packages\\deepface\\DeepFace.py:67\u001b[39m, in \u001b[36mbuild_model\u001b[39m\u001b[34m(model_name, task)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m, task: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mfacial_recognition\u001b[39m\u001b[33m\"\u001b[39m) -> Any:\n\u001b[32m     52\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    This function builds a pre-trained model\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \u001b[33;03m        built_model\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodeling\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev.Solanki\\Desktop\\CV Project\\env\\Lib\\site-packages\\deepface\\modules\\modeling.py:96\u001b[39m, in \u001b[36mbuild_model\u001b[39m\u001b[34m(task, model_name)\u001b[39m\n\u001b[32m     94\u001b[39m model = models[task].get(model_name)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     cached_models[task][model_name] = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid model_name passed - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev.Solanki\\Desktop\\CV Project\\env\\Lib\\site-packages\\deepface\\models\\facial_recognition\\VGGFace.py:45\u001b[39m, in \u001b[36mVggFaceClient.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_name = \u001b[33m\"\u001b[39m\u001b[33mVGG-Face\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_shape = (\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev.Solanki\\Desktop\\CV Project\\env\\Lib\\site-packages\\deepface\\models\\facial_recognition\\VGGFace.py:158\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m    153\u001b[39m base_model_output = Flatten()(model.layers[-\u001b[32m5\u001b[39m].output)\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# keras backend's l2 normalization layer troubles some gpu users (e.g. issue 957, 966)\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# base_model_output = Lambda(lambda x: K.l2_normalize(x, axis=1), name=\"norm_layer\")(\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m#     base_model_output\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m vgg_face_descriptor = Model(inputs=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m, outputs=base_model_output)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vgg_face_descriptor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev.Solanki\\Desktop\\CV Project\\env\\Lib\\site-packages\\keras\\src\\ops\\operation.py:276\u001b[39m, in \u001b[36mOperation.input\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    268\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[32m    269\u001b[39m \n\u001b[32m    270\u001b[39m \u001b[33;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m \u001b[33;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_tensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev.Solanki\\Desktop\\CV Project\\env\\Lib\\site-packages\\keras\\src\\ops\\operation.py:307\u001b[39m, in \u001b[36mOperation._get_node_attribute_at_index\u001b[39m\u001b[34m(self, node_index, attr, attr_name)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[32m    292\u001b[39m \n\u001b[32m    293\u001b[39m \u001b[33;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    304\u001b[39m \u001b[33;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inbound_nodes:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    308\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has never been called \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes) > node_index:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    313\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at node \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    314\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but the operation has only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    315\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inbound nodes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: The layer sequential_148 has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "# === Load model only once ===\n",
    "print(\"[INFO] Loading model...\")\n",
    "model = DeepFace.build_model(model_name)\n",
    "\n",
    "# === Utility function to check valid image ===\n",
    "def is_valid_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    return img is not None and len(img.shape) == 3  # Must be RGB/BGR\n",
    "\n",
    "# === Load embeddings ===\n",
    "print(\"[INFO] Loading face embeddings from DB...\")\n",
    "database = {}\n",
    "\n",
    "for person_folder in os.listdir(face_db_path):\n",
    "    person_path = os.path.join(face_db_path, person_folder)\n",
    "    if not os.path.isdir(person_path): continue\n",
    "\n",
    "    for img_file in os.listdir(person_path):\n",
    "        if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(person_path, img_file)\n",
    "\n",
    "        # Check if the image is valid\n",
    "        if not is_valid_image(img_path):\n",
    "            print(f\"[SKIPPED] {img_file} - Invalid or grayscale image\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            embedding_objs = DeepFace.represent(\n",
    "                img_path=img_path,\n",
    "                model_name=model_name,\n",
    "                model=model,  # Explicitly pass preloaded model\n",
    "                enforce_detection=False  # Disable strict face detection\n",
    "            )\n",
    "            rep = embedding_objs[0][\"embedding\"]\n",
    "            database.setdefault(person_folder, []).append(rep)\n",
    "            print(f\"[LOADED] {person_folder}/{img_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIPPED] {img_file} - {e}\")\n",
    "\n",
    "if not database:\n",
    "    print(\"[FATAL] No face data found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757be3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Init MediaPipe ===\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06391402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Activity Logging CSV ===\n",
    "csv_file = \"activity_log.csv\"\n",
    "if not os.path.exists(csv_file):\n",
    "    pd.DataFrame(columns=[\"Time\", \"Person\", \"Activity\"]).to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Start Video ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb.flags.writeable = False\n",
    "        results = pose.process(rgb)\n",
    "        rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        detected_name = \"Detecting...\"\n",
    "        activity = \"Unknown\"\n",
    "\n",
    "        try:\n",
    "            face_objs = DeepFace.extract_faces(frame, enforce_detection=False)\n",
    "            for face in face_objs:\n",
    "                x, y, w, h = face['facial_area'].values()\n",
    "                cropped_face = face[\"face\"]\n",
    "                emb = DeepFace.represent(cropped_face, model_name=model_name, enforce_detection=False)[0][\"embedding\"]\n",
    "                best_match, best_dist = \"Unknown\", 1.0\n",
    "\n",
    "                for name, embeddings in database.items():\n",
    "                    for ref_emb in embeddings:\n",
    "                        dist = cosine(emb, ref_emb)\n",
    "                        if dist < threshold and dist < best_dist:\n",
    "                            best_match, best_dist = name, dist\n",
    "\n",
    "                if best_dist < last_distance:\n",
    "                    last_label = best_match\n",
    "                    last_distance = best_dist\n",
    "                else:\n",
    "                    last_distance += smoothing_decay\n",
    "                    if last_distance > threshold:\n",
    "                        last_label = \"Detecting...\"\n",
    "\n",
    "                detected_name = last_label\n",
    "                color = (0, 255, 0) if last_label != \"Unknown\" else (0, 0, 255)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                cv2.putText(frame, last_label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "                break  # only one face\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Face Detection Error]: {e}\")\n",
    "\n",
    "        # === Pose Estimation + Activity Detection ===\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            lh, lk, ls = landmarks[mp_pose.PoseLandmark.LEFT_HIP], landmarks[mp_pose.PoseLandmark.LEFT_KNEE], landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "            l_hand, r_hand = landmarks[mp_pose.PoseLandmark.LEFT_WRIST], landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "            hip_knee_diff = abs(lh.y - lk.y)\n",
    "            shoulder_hip_diff = abs(ls.y - lh.y)\n",
    "            hand_diff = abs(l_hand.x - r_hand.x)\n",
    "            pose_ratio = hip_knee_diff / (shoulder_hip_diff + 1e-5)\n",
    "\n",
    "            if pose_ratio < 0.6 and hand_diff < 0.1:\n",
    "                posture, idle = \"Sitting\", True\n",
    "            elif pose_ratio < 0.6:\n",
    "                posture, idle = \"Sitting\", False\n",
    "            elif hand_diff < 0.1 and shoulder_hip_diff < 0.65:\n",
    "                posture, idle = \"Standing\", True\n",
    "            else:\n",
    "                posture, idle = \"Standing\", False\n",
    "\n",
    "            working = False\n",
    "            if prev_left_hand and prev_right_hand:\n",
    "                l_move = abs(l_hand.x - prev_left_hand[0]) + abs(l_hand.y - prev_left_hand[1])\n",
    "                r_move = abs(r_hand.x - prev_right_hand[0]) + abs(r_hand.y - prev_right_hand[1])\n",
    "                if l_move > 0.02 or r_move > 0.02:\n",
    "                    working = True\n",
    "\n",
    "            prev_left_hand = (l_hand.x, l_hand.y)\n",
    "            prev_right_hand = (r_hand.x, r_hand.y)\n",
    "\n",
    "            if idle and not working:\n",
    "                activity = f\"{posture} + Idle\"\n",
    "            elif working:\n",
    "                activity = f\"{posture} + Working\"\n",
    "            else:\n",
    "                activity = posture\n",
    "\n",
    "            activity_queue.append(activity)\n",
    "            stable_activity = statistics.mode(activity_queue)\n",
    "        else:\n",
    "            stable_activity = \"No Person Detected\"\n",
    "\n",
    "        # === Display Info ===\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        cv2.putText(frame, f\"{detected_name}: {stable_activity}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "        # === Logging ===\n",
    "        if detected_name not in [\"Unknown\", \"Detecting...\"] and stable_activity != \"No Person Detected\":\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            log_row = pd.DataFrame([[timestamp, detected_name, stable_activity]], columns=[\"Time\", \"Person\", \"Activity\"])\n",
    "            log_row.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "        cv2.imshow(\"Worker Monitoring\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
